# ğŸŒŸ RoPE: Rotary Positional Encoding Implementation

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

> **Complete implementation of Rotary Positional Encoding (RoPE) - the technique powering modern language models like GPT-4, LLaMA, ChatGPT, and Mistral 7B!**

[![YouTube Tutorial](https://img.shields.io/badge/YouTube-Tutorial-red?logo=youtube)](https://youtube.com/watch?v=YOUR_VIDEO_ID)

**ğŸ¥ [Watch the Full Tutorial on YouTube](https://youtube.com/watch?v=YOUR_VIDEO_ID)**



## ğŸ¯ What is RoPE?

**Rotary Positional Encoding (RoPE)** is a method to inject positional information into transformers by rotating query and key vectors in multi-head attention. Unlike traditional additive positional encoding, RoPE uses **multiplicative rotations** that provide:

- âœ… **Better extrapolation** to longer sequences
- âœ… **Maintained relative positions** through geometric rotation  
- âœ… **Improved context handling** compared to sinusoidal encoding
- âœ… **Mathematical elegance** using complex exponentials

## ğŸ”¥ Key Features

### ğŸ†š RoPE vs Traditional Positional Encoding

| Aspect | Traditional (Sinusoidal) | RoPE (Rotary) |
|--------|-------------------------|---------------|
| **Type** | Additive (added to embeddings) | Multiplicative (applied via rotation) |
| **Implementation** | Add sinusoidal vectors | Rotate Q/K vectors using complex exponentials |
| **Extrapolation** | Limited beyond training length | Better generalization to unseen positions |
| **Context Scaling** | Poor beyond training length | Scales better to longer sequences |
| **Mathematical Form** | `e_pos = sin/cos(pos Â· freq)` | `RoPE(x,pos) = R(pos) Â· x` |
| **Used In** | Original Transformer | GPT-NeoX, GPT-J, LLaMA, ChatGPT |

## ğŸ—ï¸ Architecture Integration

RoPE seamlessly integrates into the **self-attention mechanism**:

```
Input Embeddings â†’ RMS Norm â†’ Self-Attention (with RoPE) â†’ Feed Forward â†’ Output
                                    â†“
                    Q_rot, K_rot â† RoPE(Q, K, position)
```

## ğŸ“Š Mathematical Foundation

### Complex Rotation
```math
R_Î¸(z) = z Â· e^{iÎ¸} = z Â· (cos Î¸ + i sin Î¸)
```

### 2D Rotation Matrix
```math
R_Î¸ = [cos Î¸  -sin Î¸]
      [sin Î¸   cos Î¸]
```

### RoPE Formula
```math
RoPE(x_i, pos) = R(pos Â· Î¸_i) Â· x_i
```

Where `Î¸_i = 10000^(-2i/d)` for dimension pairs.




### Memory Optimization

- **Precomputed frequencies**: Stored once and reused
- **In-place rotations**: Minimal memory overhead
- **Block-diagonal structure**: Leverages sparsity



## ğŸ“Š Models Using RoPE

| Model | Organization | Parameters | Context Length |
|-------|-------------|------------|----------------|
| **GPT-J** | EleutherAI | 6B | 2048 |
| **GPT-NeoX** | EleutherAI | 20B | 2048 |
| **LLaMA** | Meta | 7B-65B | 2048-4096 |
| **ChatGPT** | OpenAI | Unknown | 4096+ |
| **Mistral 7B** | Mistral AI | 7B | 8192 |

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md).



## ğŸ“š References & Research

### ğŸ“„ Key Papers
- **RoFormer: Enhanced Transformer with Rotary Position Embedding** - Su et al. (2021)
- **Attention Is All You Need** - Vaswani et al. (2017)
- **GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model** - EleutherAI
- **LLaMA: Open and Efficient Foundation Language Models** - Touvron et al. (2023)

### ğŸ”— Useful Links
- [Original RoFormer Paper](https://arxiv.org/abs/2104.09864)
- [EleutherAI GPT-J Implementation](https://github.com/kingoflolz/mesh-transformer-jax)
- [LLaMA Model Card](https://github.com/facebookresearch/llama)
- [Mistral 7B Technical Report](https://arxiv.org/abs/2310.06825)

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.


## ğŸ“ Support & Contact

---

â­ **If this implementation helped you, please give it a star!** â­

![RoPE Visualization](images/rope_visualization.png)

*RoPE rotation visualization showing how position encoding is applied through geometric rotation in the embedding space.*
